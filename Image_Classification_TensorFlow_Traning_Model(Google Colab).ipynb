{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification Model Training using TensorFlow by Anurug Upascha\n",
        "## Dog vs Cat Classification with Keras Applications"
      ],
      "metadata": {
        "id": "e3Kab7alqsvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "...\n",
        "```\n",
        "Cell นี้ทำการ import libraries ที่จำเป็นสำหรับการสร้างและฝึกโมเดล เช่น TensorFlow, Keras, NumPy และ libraries สำหรับการแสดงผล"
      ],
      "metadata": {
        "id": "gthRA55Qqw8W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvKOE1eY2KM3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import seaborn as sns\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import *\n",
        "import os\n",
        "print(\"tf Version = \",tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Connect Gdrive\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "workspace_path = '/content/drive/MyDrive/TensorflowWorkspace'\n",
        "\n",
        "...\n",
        "```\n",
        "Cell นี้ทำการ Connect Gdrive เพื่อที่จะดึงข้อมูล Dataset จาก Google Drive โดย\n",
        "TensorflowWorkspace คือ Folder ที่จะเก็บทุกอย่างที่เกี่ยวกับการเทรนโมเดล"
      ],
      "metadata": {
        "id": "ucfS54-lq1s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "workspace_path = '/content/drive/MyDrive/TensorflowWorkspace'\n",
        "\n",
        "if not os.path.exists(workspace_path):\n",
        "  print(f\"Error: Workspace path '{workspace_path}' does not exist. Please create it or provide a valid path.\")\n",
        "else:\n",
        "  print(f\"Successfully mounted Google Drive and set workspace path to '{workspace_path}'\")"
      ],
      "metadata": {
        "id": "j-rLrNqv2yL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Unzip Dataset\n",
        "```python\n",
        "import zipfile\n",
        "\n",
        "datasets_path = os.path.join(workspace_path, 'datasets')\n",
        "\n",
        "...\n",
        "```\n",
        "Cell นี้ทำการโหลด Dataset จาก Google Drive มาแล้วแตกไฟล์ลงใน Env ของ Colab"
      ],
      "metadata": {
        "id": "NkbqU6hGrOm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "datasets_path = os.path.join(workspace_path, 'datasets')\n",
        "\n",
        "if not os.path.exists(datasets_path):\n",
        "  print(f\"Error: Datasets path '{datasets_path}' does not exist. Please create it or provide a valid path.\")\n",
        "else:\n",
        "  print(f\"Datasets path is '{datasets_path}'\")\n",
        "  zip_file_path = os.path.join(datasets_path, 'dogcat.zip')\n",
        "  if not os.path.exists(zip_file_path):\n",
        "    print(f\"Error: Dataset zip file '{zip_file_path}' not found.\")\n",
        "  else:\n",
        "    print(f\"Found dataset zip file at '{zip_file_path}'\")\n",
        "\n",
        "    extract_path = '/content/datasets'\n",
        "    if not os.path.exists(extract_path):\n",
        "      os.makedirs(extract_path)\n",
        "    try:\n",
        "      with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "      print(f\"Successfully extracted dataset to '{extract_path}'\")\n",
        "    except zipfile.BadZipFile:\n",
        "      print(f\"Error: '{zip_file_path}' is not a valid zip file.\")\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred during extraction: {e}\")"
      ],
      "metadata": {
        "id": "A5KLjMTU7LFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Configuration Setup\n",
        "```python\n",
        "batch_size = 24\n",
        "...\n",
        "```\n",
        "Cell นี้กำหนดค่า parameters พื้นฐานสำหรับการฝึกโมเดล:\n",
        "- ขนาด batch size\n",
        "- ขนาดของรูปภาพ input\n",
        "- ตำแหน่งของ dataset"
      ],
      "metadata": {
        "id": "mQG8e6VgrofP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "data_dir = '/content/datasets/dogcat/'"
      ],
      "metadata": {
        "id": "ZPSUUiHw3Nu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Load Training Dataset\n",
        "```python\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "...\n",
        "```\n",
        "Cell นี้โหลดข้อมูลสำหรับการฝึกฝน (training set) โดยแบ่ง 70% ของข้อมูลทั้งหมด"
      ],
      "metadata": {
        "id": "4wWxWGHkru51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,  # First split: 70% training, 30% for val+test\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "v9uDCa563bOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Load Validation and Test Datasets\n",
        "```python\n",
        "remaining_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "...\n",
        "```\n",
        "Cell นี้โหลดข้อมูลส่วนที่เหลือ 30% เพื่อนำไปแบ่งเป็น validation และ test sets"
      ],
      "metadata": {
        "id": "J5ScTDvPrwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remaining_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,  # Taking the remaining 30%\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "JPcf-RDy8N_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Split Validation and Test Sets\n",
        "```python\n",
        "val_batches = tf.data.experimental.cardinality(remaining_ds) // 2\n",
        "...\n",
        "```\n",
        "Cell นี้แบ่งข้อมูลที่เหลือออกเป็น validation set และ test set อย่างละครึ่ง\n",
        "\n",
        "```python\n",
        "val_ds = remaining_ds.take(val_batches)\n",
        "test_ds = remaining_ds.skip(val_batches)\n",
        "```\n",
        "\n",
        "\n",
        "*   take(val_batches) คือการนำข้อมูล n batch แรกจาก remaining_ds มาเป็น validation set\n",
        "*   ถ้า val_batches = 10 จะเป็นการนำ 10 batch แรกมาเป็น validation set\n",
        "*   skip(val_batches) คือการข้ามข้อมูล n batch แรก แล้วนำข้อมูลที่เหลือมาเป็น test set\n",
        "*   ถ้า val_batches = 10 จะเป็นการข้าม 10 batch แรก แล้วนำข้อมูลที่เหลือมาเป็น test set\n"
      ],
      "metadata": {
        "id": "lOmM-FJOr2AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_batches = tf.data.experimental.cardinality(remaining_ds) // 2\n",
        "test_batches = val_batches"
      ],
      "metadata": {
        "id": "_z5Np-zV825N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = remaining_ds.take(val_batches)\n",
        "test_ds = remaining_ds.skip(val_batches)"
      ],
      "metadata": {
        "id": "Kc786lx985zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Get Class Information\n",
        "```python\n",
        "class_names = train_ds.class_names\n",
        "...\n",
        "```\n",
        "Cell นี้แสดงข้อมูลของ classes ที่ใช้ในการจำแนก (dog และ cat) และจำนวน classes"
      ],
      "metadata": {
        "id": "i-A3-vn9sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print('ชื่อคลาส: ',class_names)\n",
        "print('จำนวนคลาส: ',num_classes)"
      ],
      "metadata": {
        "id": "HTeAUf-J_uMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Visualize Sample Images\n",
        "```python\n",
        "plt.figure(figsize=(10, 10))\n",
        "...\n",
        "```\n",
        "Cell นี้แสดงตัวอย่างรูปภาพจาก training set พร้อมกับ labels"
      ],
      "metadata": {
        "id": "dy6lk85BsMWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "aXt0V5Y99Iqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Optimize Dataset Performance\n",
        "```python\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "...\n",
        "```\n",
        "Cell นี้ปรับแต่ง dataset pipeline เพื่อเพิ่มประสิทธิภาพในการฝึกโมเดล\n"
      ],
      "metadata": {
        "id": "0YBIpFPcsTFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "AE9xoggK9NSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 8. Normalize Image Data\n",
        "```python\n",
        "normalization_layer = layers.Rescaling(1./224)\n",
        "...\n",
        "```\n",
        "Cell นี้ทำการ normalize ข้อมูลรูปภาพให้อยู่ในช่วง [0,1]\n",
        "\n",
        "#### การ Normalize ข้อมูลรูปภาพ\n",
        "\n",
        "การ normalize ข้อมูลรูปภาพเป็นขั้นตอนสำคัญในการเตรียมข้อมูลก่อนป้อนเข้าสู่โมเดล deep learning โดยเป็นการปรับค่าพิกเซลให้อยู่ในช่วง [0,1]\n",
        "\n",
        "#### รูปแบบการเก็บค่าพิกเซลในรูปภาพ\n",
        "\n",
        "รูปภาพดิจิตอลปกติจะเก็บค่าแต่ละพิกเซลในช่วง 0-255 โดย:\n",
        "* 0 = สีดำ\n",
        "* 255 = สีขาว\n",
        "* ค่าระหว่าง 0-255 = เฉดสีต่างๆ\n",
        "\n",
        "#### เหตุผลที่ต้อง Normalize\n",
        "\n",
        "โมเดล deep learning มักทำงานได้ดีกับข้อมูลที่มีขนาดเล็ก การใช้ค่าพิกเซล 0-255 โดยตรงอาจก่อให้เกิดปัญหา:\n",
        "* การคำนวณใช้เวลานาน\n",
        "* การเรียนรู้ของโมเดลไม่เสถียร\n",
        "* เกิดปัญหา gradient explosion\n",
        "\n",
        "#### วิธีการ Normalize\n",
        "\n",
        "```python\n",
        "# สร้าง normalization layer\n",
        "normalization_layer = layers.Rescaling(1./224)\n",
        "\n",
        "# ใช้ normalize ข้อมูล\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "```\n",
        "\n",
        "#### ตัวอย่างการแปลงค่า\n",
        "\n",
        "| ค่าพิกเซลเดิม | การคำนวณ | ค่าหลัง Normalize |\n",
        "|-------------|----------|------------------|\n",
        "| 0 | 0/224 | 0.0 |\n",
        "| 128 | 128/224 | 0.57 |\n",
        "| 255 | 255/224 | 1.0 |\n",
        "\n",
        "#### การตรวจสอบผลลัพธ์\n",
        "\n",
        "```python\n",
        "# ตรวจสอบค่าต่ำสุดและสูงสุดหลัง normalize\n",
        "print(np.min(first_image), np.max(first_image))\n",
        "```\n",
        "\n",
        "#### ประโยชน์ของการ Normalize\n",
        "\n",
        "1. โมเดลเรียนรู้ได้ดีขึ้น\n",
        "2. ลดปัญหา vanishing/exploding gradients\n",
        "3. เพิ่มความเร็วในการเทรนโมเดล\n",
        "4. เปรียบเทียบระหว่างรูปภาพได้ง่ายขึ้น\n",
        "\n",
        "#### ข้อควรระวัง\n",
        "\n",
        "1. ต้อง normalize ทุกชุดข้อมูลด้วยวิธีเดียวกัน:\n",
        "   * Training set\n",
        "   * Validation set\n",
        "   * Test set\n",
        "2. เก็บค่าที่ใช้ normalize (scaling factor) ไว้สำหรับข้อมูลใหม่\n",
        "3. ตรวจสอบค่าหลัง normalize ว่าอยู่ในช่วง [0,1] จริง\n",
        "\n",
        "## Note\n",
        "\n",
        "การ normalize เป็นเพียงขั้นตอนหนึ่งในการ preprocess ข้อมูล อาจต้องใช้ร่วมกับเทคนิคอื่นๆ เช่น:\n",
        "* Data augmentation\n",
        "* Standardization\n",
        "* Feature scaling\n",
        "\n",
        "เพื่อให้ได้ผลลัพธ์ที่ดีที่สุดในการฝึกโมเดล\n"
      ],
      "metadata": {
        "id": "HagXQfuOsX8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = layers.Rescaling(1./224)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "metadata": {
        "id": "Cya-pHpu9XML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 9. Verify Normalization\n",
        "```python\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "...\n",
        "```\n",
        "Cell นี้ตรวจสอบว่าการ normalize ทำงานถูกต้อง"
      ],
      "metadata": {
        "id": "n3cwO--6seLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ],
      "metadata": {
        "id": "SqOavrfA-fdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Create Model Architecture\n",
        "```python\n",
        "num_classes = len(class_names)\n",
        "...\n",
        "```\n",
        "Cell นี้สร้างโครงสร้างโมเดลโดยใช้ Keras Applications และเพิ่ม custom layers\n",
        "\n",
        "#### Available Models\n",
        "\n",
        "| Model                | Size (MB) | Top-1 Accuracy | Top-5 Accuracy | Parameters | Depth | CPU Time (ms) | GPU Time (ms) |\n",
        "|----------------------|----------|---------------|---------------|------------|-------|--------------|--------------|\n",
        "| Xception            | 88       | 79.0%         | 94.5%         | 22.9M      | 81    | 109.4        | 8.1          |\n",
        "| VGG16               | 528      | 71.3%         | 90.1%         | 138.4M     | 16    | 69.5         | 4.2          |\n",
        "| VGG19               | 549      | 71.3%         | 90.0%         | 143.7M     | 19    | 84.8         | 4.4          |\n",
        "| ResNet50            | 98       | 74.9%         | 92.1%         | 25.6M      | 107   | 58.2         | 4.6          |\n",
        "| ResNet50V2         | 98       | 76.0%         | 93.0%         | 25.6M      | 103   | 45.6         | 4.4          |\n",
        "| ResNet101          | 171      | 76.4%         | 92.8%         | 44.7M      | 209   | 89.6         | 5.2          |\n",
        "| ResNet101V2        | 171      | 77.2%         | 93.8%         | 44.7M      | 205   | 72.7         | 5.4          |\n",
        "| ResNet152          | 232      | 76.6%         | 93.1%         | 60.4M      | 311   | 127.4        | 6.5          |\n",
        "| ResNet152V2        | 232      | 78.0%         | 94.2%         | 60.4M      | 307   | 107.5        | 6.6          |\n",
        "| InceptionV3        | 92       | 77.9%         | 93.7%         | 23.9M      | 189   | 42.2         | 6.9          |\n",
        "| InceptionResNetV2  | 215      | 80.3%         | 95.3%         | 55.9M      | 449   | 130.2        | 10.0         |\n",
        "| MobileNet          | 16       | 70.4%         | 89.5%         | 4.3M       | 55    | 22.6         | 3.4          |\n",
        "| MobileNetV2        | 14       | 71.3%         | 90.1%         | 3.5M       | 105   | 25.9         | 3.8          |\n",
        "| DenseNet121        | 33       | 75.0%         | 92.3%         | 8.1M       | 242   | 77.1         | 5.4          |\n",
        "| DenseNet169        | 57       | 76.2%         | 93.2%         | 14.3M      | 338   | 96.4         | 6.3          |\n",
        "| DenseNet201        | 80       | 77.3%         | 93.6%         | 20.2M      | 402   | 127.2        | 6.7          |\n",
        "| NASNetMobile       | 23       | 74.4%         | 91.9%         | 5.3M       | 389   | 27.0         | 6.7          |\n",
        "| NASNetLarge        | 343      | 82.5%         | 96.0%         | 88.9M      | 533   | 344.5        | 20.0         |\n",
        "| EfficientNetB0     | 29       | 77.1%         | 93.3%         | 5.3M       | 132   | 46.0         | 4.9          |\n",
        "| EfficientNetB1     | 31       | 79.1%         | 94.4%         | 7.9M       | 186   | 60.2         | 5.6          |\n",
        "| EfficientNetB2     | 36       | 80.1%         | 94.9%         | 9.2M       | 186   | 80.8         | 6.5          |\n",
        "| EfficientNetB3     | 48       | 81.6%         | 95.7%         | 12.3M      | 210   | 140.0        | 8.8          |\n",
        "| EfficientNetB4     | 75       | 82.9%         | 96.4%         | 19.5M      | 258   | 308.3        | 15.1         |\n",
        "| EfficientNetB5     | 118      | 83.6%         | 96.7%         | 30.6M      | 312   | 579.2        | 25.3         |\n",
        "| EfficientNetB6     | 166      | 84.0%         | 96.8%         | 43.3M      | 360   | 958.1        | 40.4         |\n",
        "| EfficientNetB7     | 256      | 84.3%         | 97.0%         | 66.7M      | 438   | 1578.9       | 61.6         |\n",
        "| ConvNeXtTiny      | 109.42   | 81.3%         | -             | 28.6M      | -     | -            | -            |\n",
        "| ConvNeXtSmall     | 192.29   | 82.3%         | -             | 50.2M      | -     | -            | -            |\n",
        "| ConvNeXtBase      | 338.58   | 85.3%         | -             | 88.5M      | -     | -            | -            |\n",
        "| ConvNeXtLarge     | 755.07   | 86.3%         | -             | 197.7M     | -     | -            | -            |\n",
        "| ConvNeXtXLarge    | 1310     | 86.7%         | -             | 350.1M     | -     | -            | -            |\n",
        "\n"
      ],
      "metadata": {
        "id": "6xFUSZGAsiR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# จำนวนคลาสในงานจำแนกประเภท\n",
        "num_classes = len(class_names)\n",
        "# สร้าง input layer สำหรับรูปภาพขนาด (img_height, img_width, 3)\n",
        "inputs = Input(shape=(img_height, img_width, 3))\n",
        "\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False)\n",
        "base_model.trainable = False  # Freeze layers ของ EfficientNetB0\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = Flatten()(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x) # Output layer สำหรับจำแนกจำนวนคลาส\n",
        "\n",
        "# สร้างโมเดลโดยกำหนด inputs และ outputs\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# คอมไพล์โมเดล\n",
        "model.compile(optimizer='adamw',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jjwckRkP_K1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Train Model\n",
        "```python\n",
        "model_save_path = 'models/EfficientNetB0_dogcat_10.keras'\n",
        "...\n",
        "```\n",
        "Cell นี้ทำการฝึกโมเดลพร้อมกับบันทึกโมเดลที่ดีที่สุด"
      ],
      "metadata": {
        "id": "tyB74Y1Pst3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'models/dogcat.keras'\n",
        "\n",
        "epochs = 10\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=model_save_path,  # ตำแหน่งไฟล์\n",
        "    monitor='val_loss',       # เมตริกที่ใช้ในการตรวจสอบ (เช่น val_loss หรือ val_accuracy)\n",
        "    save_best_only=True,      # บันทึกเฉพาะโมเดลที่ดีที่สุด\n",
        "    save_weights_only=False,  # บันทึกทั้งโมเดล (ไม่ใช่แค่ weights)\n",
        "    mode='min',               # เลือกโมเดลที่มีค่าต่ำสุดสำหรับ val_loss\n",
        "    verbose=1                 # แสดงข้อความเมื่อบันทึก\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "cip6242vA7rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Evaluate Model\n",
        "```python\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "...\n",
        "```\n",
        "Cell นี้ประเมินประสิทธิภาพของโมเดลบน test set"
      ],
      "metadata": {
        "id": "lDWKV85LswmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "pPxnburDBKOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Plot Training History\n",
        "```python\n",
        "acc = history.history['accuracy']\n",
        "...\n",
        "```\n",
        "Cell นี้แสดงกราฟของ accuracy และ loss ระหว่างการฝึกโมเดล\n",
        "\n",
        "### 14. Create Confusion Matrix\n",
        "```python\n",
        "y_pred = model.predict(test_ds)\n",
        "...\n",
        "```\n",
        "Cell นี้สร้าง confusion matrix เพื่อแสดงผลการทำนายของโมเดล\n",
        "\n",
        "### 15. Generate Performance Curves\n",
        "```python\n",
        "y_pred = model.predict(test_ds)\n",
        "...\n",
        "```\n",
        "Cell นี้สร้าง Precision-Recall curve และ F1 curve เพื่อแสดงประสิทธิภาพของโมเดลในรูปแบบต่างๆ"
      ],
      "metadata": {
        "id": "78QUBeXps0wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig('validpic/training_vs_validation.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mGmqy3sFBRRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_ds)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "y_true = []\n",
        "for _, labels in test_ds:\n",
        "    y_true.extend(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('validpic/confusion_matrix.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SqSseAWXZNb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_ds)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "y_true = []\n",
        "for _, labels in test_ds:\n",
        "    y_true.extend(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "average_precision = dict()\n",
        "for i in range(num_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(y_true == i, y_pred[:, i])\n",
        "    average_precision[i] = average_precision_score(y_true == i, y_pred[:, i])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(num_classes):\n",
        "    plt.plot(recall[i], precision[i], lw=2, label=f'{class_names[i]} (Average = {average_precision[i]:.2f})')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall curve\")\n",
        "plt.legend(loc='lower left', bbox_to_anchor=(0, 1))\n",
        "plt.savefig('validpic/precision_recall_curve.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "f1 = []\n",
        "thresholds = np.linspace(0,1,101)\n",
        "for t in thresholds:\n",
        "    f1_t = f1_score(y_true, (y_pred > t).argmax(axis=1), average=\"weighted\")\n",
        "    f1.append(f1_t)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(num_classes):\n",
        "    plt.plot(recall[i], precision[i], lw=2, label=f'{class_names[i]} (Average = {average_precision[i]:.2f})')\n",
        "plt.plot(thresholds, f1, lw=2, label=\"F1 Score\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"F1\")\n",
        "plt.title(\"F1 Curve\")\n",
        "\n",
        "\n",
        "plt.legend(loc='lower left', bbox_to_anchor=(0, 1))\n",
        "plt.savefig('validpic/f1_curve.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kcZkk2owZ9SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Export Validpic and Model\n",
        "```python\n",
        "source_dir = 'validpic'\n",
        "destination_dir = '/content/drive/MyDrive/TensorflowWorkspace/validpic'\n",
        "...\n",
        "```\n",
        "```python\n",
        "model_save_path = 'models/dogcat.keras'\n",
        "destination_path = os.path.join(workspace_path, 'models', 'dogcat.keras')\n",
        "...\n",
        "```\n",
        "\n",
        "Cell นี้ทำการ Export Validpic และ Model ไปยัง Google Drive"
      ],
      "metadata": {
        "id": "HyZiOG5AtLrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = 'validpic'\n",
        "destination_dir = '/content/drive/MyDrive/TensorflowWorkspace/validpic'\n",
        "\n",
        "import os\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    for filename in os.listdir(source_dir):\n",
        "        source_file = os.path.join(source_dir, filename)\n",
        "        destination_file = os.path.join(destination_dir, filename)\n",
        "        shutil.copy2(source_file, destination_file)\n",
        "    print(f\"Successfully copied files from '{source_dir}' to '{destination_dir}'\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Source directory '{source_dir}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during file copy: {e}\")\n"
      ],
      "metadata": {
        "id": "ATm4Wu_UZyuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'models/dogcat.keras'\n",
        "destination_path = os.path.join(workspace_path, 'models', 'dogcat.keras')\n",
        "\n",
        "\n",
        "models_dir_in_drive = os.path.join(workspace_path, 'models')\n",
        "os.makedirs(models_dir_in_drive, exist_ok=True)\n",
        "\n",
        "\n",
        "try:\n",
        "  shutil.copy(model_save_path, destination_path)\n",
        "  print(f\"Successfully copied '{model_save_path}' to '{destination_path}'\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File '{model_save_path}' not found.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during file copy: {e}\")\n"
      ],
      "metadata": {
        "id": "vavzrpraENKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-N97RyRibNk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}